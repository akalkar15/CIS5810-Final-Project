{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6JXFuh9HTz24yPz4OCLLM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":47,"metadata":{"id":"KSgsAC1CK_tQ","executionInfo":{"status":"ok","timestamp":1730085146594,"user_tz":240,"elapsed":678,"user":{"displayName":"ankita kalkar","userId":"17111250717128310927"}}},"outputs":[],"source":["import numpy as np\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import torch\n","import torchvision.transforms as transforms\n","from torchvision import models\n","from PIL import Image\n","import os\n","from segment_anything import SamPredictor, sam_model_registry\n","import matplotlib.pyplot as plt\n","from scenedetect import detect, AdaptiveDetector, split_video_ffmpeg\n","from google.cloud import speech_v1p1beta1 as speech\n","from google.oauth2 import service_account\n","from pydub import AudioSegment"]},{"cell_type":"code","source":["!pip install scenedetect supervision\n","!pip install segment-anything ultralytics\n","!pip install google-cloud-speech google-auth pydub"],"metadata":{"id":"rn89w9ELGfHj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ykkq8pKOAoHp","executionInfo":{"status":"ok","timestamp":1730083501185,"user_tz":240,"elapsed":24003,"user":{"displayName":"ankita kalkar","userId":"17111250717128310927"}},"outputId":"f261f4c8-2770-4e1c-9c12-70fe4a303093"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Github/cis5810"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A_qsHm4WA1Ev","executionInfo":{"status":"ok","timestamp":1730083542308,"user_tz":240,"elapsed":356,"user":{"displayName":"ankita kalkar","userId":"17111250717128310927"}},"outputId":"32b4c4c9-b22e-4e05-c8f9-6a81596c54e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Function to determine camera shot movement (pan, tilt, zoom, or static)\n","\"\"\"\n","\n","def detect_camera_movement(video_path):\n","    cap = cv2.VideoCapture(video_path)\n","\n","    ret, prev_frame = cap.read()\n","    if not ret:\n","        print(\"Error reading video file\")\n","        return\n","\n","    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n","\n","    movement = {'pan': 0, 'tilt': 0, 'zoom': 0}\n","\n","    while True:\n","        ret, next_frame = cap.read()\n","        if not ret:\n","            break\n","\n","        next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)\n","\n","        flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None,\n","                                            0.5, 3, 15, 3, 5, 1.2, 0)\n","        # Horizontal flow\n","        flow_x = flow[..., 0]\n","        # Vertical flow\n","        flow_y = flow[..., 1]\n","\n","        mean_flow_x = np.mean(flow_x)\n","        mean_flow_y = np.mean(flow_y)\n","\n","        magnitude, angle = cv2.cartToPolar(flow_x, flow_y, angleInDegrees=True)\n","        mean_magnitude = np.mean(magnitude)\n","\n","        # Case 1: Pan\n","        if abs(mean_flow_x) > abs(mean_flow_y):\n","            movement_type = \"Pan\" if mean_flow_x > 0 else \"Reverse Pan\"\n","            movement['pan'] += 1\n","        # Case 2: Tilt (vertical movement)\n","        elif abs(mean_flow_y) > abs(mean_flow_x):\n","            movement_type = \"Tilt\" if mean_flow_y > 0 else \"Reverse Tilt\"\n","            movement['tilt'] += 1\n","        # Case 3: Zoom\n","        else:\n","            if np.median(flow_x) < 0 and np.median(flow_y) < 0:\n","                movement_type = \"Zoom In\"\n","                movement['zoom'] += 1\n","            elif np.median(flow_x) > 0 and np.median(flow_y) > 0:\n","                movement_type = \"Zoom Out\"\n","                movement['zoom'] += 1\n","            else:\n","                movement_type = \"Static/No significant movement\"\n","\n","        print(f\"Detected Movement: {movement_type}\")\n","        # Show the optical flow map (optional)\n","        hsv = np.zeros_like(prev_frame)\n","        hsv[..., 1] = 255\n","        hsv[..., 0] = angle / 2  # Hue corresponds to the direction of flow\n","        hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n","        flow_map = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","        cv2_imshow(next_frame)\n","        cv2_imshow(flow_map)\n","\n","        # Update previous frame\n","        prev_gray = next_gray\n","\n","        # Break loop on 'q' key press\n","        if cv2.waitKey(30) & 0xFF == ord('q'):\n","            break\n","\n","    cap.release()\n","    cv2.destroyAllWindows()\n","\n","    print(\"Overall Movement Detected: \", movement)"],"metadata":{"id":"ItNJTpWQD5Df","executionInfo":{"status":"ok","timestamp":1730084794970,"user_tz":240,"elapsed":300,"user":{"displayName":"ankita kalkar","userId":"17111250717128310927"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["scene_list = detect('tomJerry.mp4', AdaptiveDetector())\n","split_video_ffmpeg('tomJerry.mp4', scene_list)"],"metadata":{"id":"kGb-mmWCGFoT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Load the pre-trained VGG16 model\n","vgg16 = models.vgg16(pretrained=True)\n","vgg16.eval()  # Set the model to evaluation mode\n","\n","# Define a set of class labels (this is just an example, you may need to modify it)\n","class_labels = ['Wide Shot', 'Medium Shot', 'Close-Up', 'Over-the-Shoulder', 'POV', 'Cut-In', 'Establishing Shot']\n","\n","# Define image transformations for the VGG16 input\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Function to classify an image\n","def classify_image(image):\n","    # Transform the image\n","    image = transform(image).unsqueeze(0)  # Add batch dimension\n","\n","    # Make prediction\n","    with torch.no_grad():\n","        outputs = vgg16(image)\n","        _, predicted_class = outputs.max(1)\n","\n","    # For demonstration purposes, map to custom labels (note: VGG16's original classes are ImageNet)\n","    # You would need a classifier trained for camera shot classification or use features from VGG16\n","    # and pass them to your custom classifier.\n","    label = class_labels[predicted_class.item() % len(class_labels)]\n","    return label\n","\n","# Function to extract frames from a video and classify them\n","def classify_video(video_path, frame_interval=30):\n","    # Open the video file\n","    cap = cv2.VideoCapture(video_path)\n","    frame_count = 0\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # Process every 'frame_interval' frames\n","        if frame_count % frame_interval == 0:\n","            # Convert the frame (BGR to RGB)\n","            image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","            label = classify_image(image)\n","            print(f\"Frame {frame_count}: Classified as: {label}\")\n","\n","        frame_count += 1\n","\n","    cap.release()\n","    cv2.destroyAllWindows()\n","\n","classify_video('tomJerry-Scene-001.mp4', frame_interval=30)"],"metadata":{"id":"JVpDWPEVG1BG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import supervision as sv\n","from segment_anything import SamPredictor, sam_model_registry\n","import matplotlib.pyplot as plt\n","\n","# Load the SAM model for object detection\n","sam = sam_model_registry[\"vit_b\"](\"../sam_vit_b_01ec64.pth\")\n","sam_predictor = SamPredictor(sam)\n","\n","# Function to extract frames from a video and detect objects\n","def detect_objects_in_video(video_path, frame_interval=30):\n","    # Open the video file\n","    cap = cv2.VideoCapture(video_path)\n","    frame_count = 0\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # Process every 'frame_interval' frames\n","        if frame_count % frame_interval == 0:\n","            # Set SAM input image\n","            sam_predictor.set_image(frame)\n","\n","            # Predict objects in the frame\n","            masks, _, _ = sam_predictor.predict(\n","                point_coords=None,\n","                point_labels=None,\n","                box=None,\n","                multimask_output=False\n","            )\n","\n","            # Draw the detected masks on the frame\n","            annotated_frame = frame.copy()\n","            for mask in masks:\n","                color = np.random.randint(0, 255, size=(3,), dtype=np.uint8)\n","                mask = mask.astype(bool)\n","                annotated_frame[mask] = cv2.addWeighted(annotated_frame, 0.5, np.full_like(annotated_frame, color), 0.5, 0)[mask]\n","\n","            # Convert the annotated frame to RGB (from BGR)\n","            annotated_frame_rgb = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)\n","\n","            # Display the frame using matplotlib\n","            plt.figure(figsize=(10, 6))\n","            plt.imshow(annotated_frame_rgb)\n","            plt.axis('off')\n","            plt.show()\n","\n","        frame_count += 1\n","\n","    cap.release()\n","\n","video_path = \"tomJerry-Scene-001.mp4\"\n","detect_objects_in_video(video_path, frame_interval=30)"],"metadata":{"id":"IIpa0AZ2G2zM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import supervision as sv\n","from ultralytics import YOLO\n","import numpy as np\n","\n","# Load YOLO model\n","model = YOLO('yolov8n.pt')\n","tracker = sv.ByteTrack()\n","\n","box_annotator = sv.BoxAnnotator()\n","label_annotator = sv.LabelAnnotator()\n","\n","# Callback function to process each frame\n","def callback(frame: np.ndarray, index: int) -> np.ndarray:\n","    results = model(frame)[0]\n","    detections = sv.Detections.from_ultralytics(results)\n","    detections = tracker.update_with_detections(detections)\n","\n","    labels = [f\"#{tracker_id}\" for tracker_id in detections.tracker_id]\n","\n","    annotated_frame = box_annotator.annotate(\n","        scene=frame.copy(), detections=detections)\n","    annotated_frame = label_annotator.annotate(\n","        scene=annotated_frame, detections=detections, labels=labels)\n","    return annotated_frame\n","\n","# Process video and save output\n","sv.process_video(\n","    source_path=\"tomJerry-Scene-001.mp4\",\n","    target_path=\"tomJerry-Scene-001-tracked.mp4\",\n","    callback=callback\n",")"],"metadata":{"id":"VdUftXbuG44z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert mp4 to wav\n","!ffmpeg -i hunger_games_scene_2.mp4 -ab 160k -ac 2 -ar 44100 -vn hunger_games_scene_2.wav"],"metadata":{"id":"joKw-YpiG-DH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","credentials = service_account.Credentials.from_service_account_file('/content/cis5810-speech-sa-key.json')\n","client = speech.SpeechClient(credentials=credentials)\n","\n","# Path to your audio file\n","audio_file = 'MyDrive/CIS5810/Videos/hunger_games_scene_2.wav'\n","\n","# Convert stereo to mono using pydub\n","sound = AudioSegment.from_wav(audio_file)\n","sound = sound.set_channels(1)  # Convert to mono\n","mono_audio_file = 'MyDrive/CIS5810/Videos/hunger_games_scene_2_mono.wav'\n","sound.export(mono_audio_file, format=\"wav\")\n","\n","with open(mono_audio_file, 'rb') as audio:\n","    content = audio.read()\n","\n","audio = speech.RecognitionAudio(content=content)\n","config = speech.RecognitionConfig(\n","    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n","    sample_rate_hertz=44100,\n","    language_code='en-US'\n",")\n","\n","response = client.recognize(config=config, audio=audio)\n","\n","for result in response.results:\n","    print(result.alternatives[0].transcript)"],"metadata":{"id":"EulJgapZG_m1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! git add ."],"metadata":{"id":"X2bpN4ZSHIm_","executionInfo":{"status":"ok","timestamp":1730085260762,"user_tz":240,"elapsed":330,"user":{"displayName":"ankita kalkar","userId":"17111250717128310927"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["! git commit -m \"added Tim's experiments\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5MEbCIDLHcye","executionInfo":{"status":"ok","timestamp":1730085275853,"user_tz":240,"elapsed":889,"user":{"displayName":"ankita kalkar","userId":"17111250717128310927"}},"outputId":"df7258f9-f1e8-45f5-a19f-39ee7394880c"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch master\n","Your branch is up to date with 'cis5810/master'.\n","\n","nothing to commit, working tree clean\n"]}]},{"cell_type":"code","source":["! f"],"metadata":{"id":"cCWvVYa3HkQI"},"execution_count":null,"outputs":[]}]}